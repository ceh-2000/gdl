{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e457c8-78cc-4d96-a904-53f63f77ce9a",
   "metadata": {},
   "source": [
    "# testing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505e0696-bf03-4cf3-8cc9-f276297f8eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (run_baseline.sh, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m~\\OneDrive - University of Cambridge\\Courses Material\\U L65 Geometric Deep Learning\\Project\\NeuroGraph_code\\run_baseline.sh:6\u001b[1;36m\u001b[0m\n\u001b[1;33m    python $main --dataset $dataset --model $model --device 'cuda' --batch_size $batch_size --runs 10\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "parser.add_argument('--dataset', type=str, default='HCPGender')\n",
    "parser.add_argument('--runs', type=int, default=1)\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "parser.add_argument('--seed', type=int, default=123)\n",
    "parser.add_argument('--model', type=str, default=\"GCNConv\")\n",
    "parser.add_argument('--hidden', type=int, default=32)\n",
    "parser.add_argument('--hidden_mlp', type=int, default=64)\n",
    "parser.add_argument('--num_layers', type=int, default=3)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--echo_epoch', type=int, default=50)\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--early_stopping', type=int, default=50)\n",
    "parser.add_argument('--lr', type=float, default=1e-5)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0005)\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c200c6b-5cf1-41b1-a241-7f89640c6871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1078\n",
      "dataset loaded successfully! HCPGender\n",
      "dataset HCPGender loaded with train 754 val 108 test 216 splits\n",
      "ResidualAttentionalGNN1(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1000, 32)\n",
      "    (1-2): 2 x GCNConv(32, 32)\n",
      "  )\n",
      "  (attention_layers): ModuleList(\n",
      "    (0-2): 3 x Attention1(\n",
      "      (linear): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (aggr): MeanAggregation()\n",
      "  (bn): BatchNorm1d(998504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=998600, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters is: 65948434\n",
      "epoch: 0, loss: 0.047242, val_acc:0.53, test_acc:0.59\n",
      "epoch: 1, loss: 0.045334, val_acc:0.68, test_acc:0.7\n",
      "epoch: 2, loss: 0.04438, val_acc:0.52, test_acc:0.58\n",
      "epoch: 3, loss: 0.044567, val_acc:0.58, test_acc:0.67\n",
      "epoch: 4, loss: 0.04328, val_acc:0.63, test_acc:0.68\n",
      "epoch: 5, loss: 0.043439, val_acc:0.69, test_acc:0.68\n",
      "epoch: 6, loss: 0.040888, val_acc:0.71, test_acc:0.75\n",
      "epoch: 7, loss: 0.041435, val_acc:0.59, test_acc:0.69\n",
      "epoch: 8, loss: 0.040814, val_acc:0.66, test_acc:0.71\n",
      "epoch: 9, loss: 0.040658, val_acc:0.64, test_acc:0.68\n",
      "epoch: 10, loss: 0.041631, val_acc:0.62, test_acc:0.69\n",
      "epoch: 11, loss: 0.040698, val_acc:0.71, test_acc:0.77\n",
      "epoch: 12, loss: 0.039607, val_acc:0.74, test_acc:0.77\n",
      "epoch: 13, loss: 0.039266, val_acc:0.69, test_acc:0.74\n",
      "epoch: 14, loss: 0.038548, val_acc:0.74, test_acc:0.74\n",
      "epoch: 15, loss: 0.037674, val_acc:0.71, test_acc:0.73\n",
      "epoch: 16, loss: 0.039, val_acc:0.78, test_acc:0.78\n",
      "epoch: 17, loss: 0.039521, val_acc:0.75, test_acc:0.76\n",
      "epoch: 18, loss: 0.037866, val_acc:0.75, test_acc:0.73\n",
      "epoch: 19, loss: 0.037863, val_acc:0.78, test_acc:0.79\n",
      "epoch: 20, loss: 0.035801, val_acc:0.75, test_acc:0.75\n",
      "epoch: 21, loss: 0.037083, val_acc:0.79, test_acc:0.81\n",
      "epoch: 22, loss: 0.036537, val_acc:0.79, test_acc:0.8\n",
      "epoch: 23, loss: 0.036903, val_acc:0.76, test_acc:0.8\n",
      "epoch: 24, loss: 0.036524, val_acc:0.83, test_acc:0.82\n",
      "epoch: 25, loss: 0.036733, val_acc:0.79, test_acc:0.84\n",
      "epoch: 26, loss: 0.035077, val_acc:0.75, test_acc:0.82\n",
      "epoch: 27, loss: 0.035031, val_acc:0.79, test_acc:0.81\n",
      "epoch: 28, loss: 0.03733, val_acc:0.72, test_acc:0.79\n",
      "epoch: 29, loss: 0.035954, val_acc:0.78, test_acc:0.8\n",
      "epoch: 30, loss: 0.036716, val_acc:0.79, test_acc:0.82\n",
      "epoch: 31, loss: 0.035126, val_acc:0.74, test_acc:0.75\n",
      "epoch: 32, loss: 0.036335, val_acc:0.82, test_acc:0.85\n",
      "epoch: 33, loss: 0.034932, val_acc:0.85, test_acc:0.85\n",
      "epoch: 34, loss: 0.034986, val_acc:0.82, test_acc:0.86\n",
      "epoch: 35, loss: 0.034609, val_acc:0.84, test_acc:0.86\n",
      "epoch: 36, loss: 0.035967, val_acc:0.81, test_acc:0.81\n",
      "epoch: 37, loss: 0.034068, val_acc:0.86, test_acc:0.86\n",
      "epoch: 38, loss: 0.033328, val_acc:0.84, test_acc:0.83\n",
      "epoch: 39, loss: 0.034653, val_acc:0.8, test_acc:0.81\n",
      "epoch: 40, loss: 0.03279, val_acc:0.8, test_acc:0.83\n",
      "epoch: 41, loss: 0.033008, val_acc:0.86, test_acc:0.84\n",
      "epoch: 42, loss: 0.032702, val_acc:0.86, test_acc:0.82\n",
      "epoch: 43, loss: 0.03325, val_acc:0.84, test_acc:0.87\n",
      "epoch: 44, loss: 0.033916, val_acc:0.81, test_acc:0.82\n",
      "epoch: 45, loss: 0.033911, val_acc:0.79, test_acc:0.84\n",
      "epoch: 46, loss: 0.033151, val_acc:0.79, test_acc:0.83\n",
      "epoch: 47, loss: 0.032846, val_acc:0.8, test_acc:0.81\n",
      "epoch: 48, loss: 0.033618, val_acc:0.84, test_acc:0.85\n",
      "epoch: 49, loss: 0.033002, val_acc:0.88, test_acc:0.84\n",
      "epoch: 50, loss: 0.031781, val_acc:0.84, test_acc:0.83\n",
      "epoch: 51, loss: 0.033119, val_acc:0.86, test_acc:0.84\n",
      "epoch: 52, loss: 0.03135, val_acc:0.81, test_acc:0.83\n",
      "epoch: 53, loss: 0.032284, val_acc:0.81, test_acc:0.84\n",
      "epoch: 54, loss: 0.032026, val_acc:0.87, test_acc:0.85\n",
      "epoch: 55, loss: 0.032492, val_acc:0.81, test_acc:0.83\n",
      "epoch: 56, loss: 0.031378, val_acc:0.81, test_acc:0.84\n",
      "epoch: 57, loss: 0.032214, val_acc:0.86, test_acc:0.84\n",
      "epoch: 58, loss: 0.031972, val_acc:0.82, test_acc:0.86\n",
      "epoch: 59, loss: 0.032596, val_acc:0.75, test_acc:0.8\n",
      "epoch: 60, loss: 0.031085, val_acc:0.8, test_acc:0.84\n",
      "epoch: 61, loss: 0.031341, val_acc:0.82, test_acc:0.84\n",
      "epoch: 62, loss: 0.031121, val_acc:0.83, test_acc:0.83\n",
      "epoch: 63, loss: 0.031527, val_acc:0.88, test_acc:0.86\n",
      "epoch: 64, loss: 0.02989, val_acc:0.81, test_acc:0.83\n",
      "epoch: 65, loss: 0.030941, val_acc:0.78, test_acc:0.84\n",
      "epoch: 66, loss: 0.030481, val_acc:0.76, test_acc:0.84\n",
      "epoch: 67, loss: 0.030859, val_acc:0.81, test_acc:0.85\n",
      "epoch: 68, loss: 0.030385, val_acc:0.83, test_acc:0.86\n",
      "epoch: 69, loss: 0.03128, val_acc:0.82, test_acc:0.86\n",
      "epoch: 70, loss: 0.030237, val_acc:0.81, test_acc:0.85\n",
      "epoch: 71, loss: 0.029984, val_acc:0.85, test_acc:0.83\n",
      "epoch: 72, loss: 0.029634, val_acc:0.82, test_acc:0.85\n",
      "epoch: 73, loss: 0.029248, val_acc:0.87, test_acc:0.88\n",
      "epoch: 74, loss: 0.029263, val_acc:0.78, test_acc:0.82\n",
      "epoch: 75, loss: 0.030826, val_acc:0.84, test_acc:0.84\n",
      "epoch: 76, loss: 0.029104, val_acc:0.83, test_acc:0.87\n",
      "epoch: 77, loss: 0.028646, val_acc:0.88, test_acc:0.85\n",
      "epoch: 78, loss: 0.029474, val_acc:0.85, test_acc:0.87\n",
      "epoch: 79, loss: 0.029531, val_acc:0.86, test_acc:0.84\n",
      "epoch: 80, loss: 0.028848, val_acc:0.85, test_acc:0.86\n",
      "epoch: 81, loss: 0.028839, val_acc:0.84, test_acc:0.86\n",
      "epoch: 82, loss: 0.029576, val_acc:0.89, test_acc:0.89\n",
      "epoch: 83, loss: 0.028492, val_acc:0.88, test_acc:0.88\n",
      "epoch: 84, loss: 0.027656, val_acc:0.84, test_acc:0.88\n",
      "epoch: 85, loss: 0.027381, val_acc:0.84, test_acc:0.85\n",
      "epoch: 86, loss: 0.027699, val_acc:0.84, test_acc:0.86\n",
      "epoch: 87, loss: 0.028285, val_acc:0.82, test_acc:0.86\n",
      "epoch: 88, loss: 0.027366, val_acc:0.84, test_acc:0.86\n",
      "epoch: 89, loss: 0.027965, val_acc:0.79, test_acc:0.83\n",
      "epoch: 90, loss: 0.027538, val_acc:0.84, test_acc:0.88\n",
      "epoch: 91, loss: 0.027743, val_acc:0.79, test_acc:0.84\n",
      "epoch: 92, loss: 0.026892, val_acc:0.82, test_acc:0.86\n",
      "epoch: 93, loss: 0.028106, val_acc:0.88, test_acc:0.84\n",
      "epoch: 94, loss: 0.028319, val_acc:0.8, test_acc:0.83\n",
      "epoch: 95, loss: 0.027464, val_acc:0.83, test_acc:0.85\n",
      "epoch: 96, loss: 0.027533, val_acc:0.84, test_acc:0.88\n",
      "epoch: 97, loss: 0.026828, val_acc:0.83, test_acc:0.85\n",
      "epoch: 98, loss: 0.026754, val_acc:0.81, test_acc:0.86\n",
      "epoch: 99, loss: 0.02607, val_acc:0.86, test_acc:0.86\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "%run main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99eb561b-fc3c-450b-8f7f-027db62a8e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1078\n",
      "dataset loaded successfully! HCPGender\n",
      "dataset HCPGender loaded with train 754 val 108 test 216 splits\n",
      "1000\n",
      "3\n",
      "ResidualAttentionalGNN1(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1000, 32)\n",
      "    (1-2): 2 x GCNConv(32, 32)\n",
      "  )\n",
      "  (attention_layers): ModuleList(\n",
      "    (0-2): 3 x Attention1(\n",
      "      (linear): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (aggr): MeanAggregation()\n",
      "  (bn): BatchNorm1d(998504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=998600, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters is: 65948434\n",
      "epoch: 0, loss: 0.047242, val_acc:0.53, test_acc:0.59\n",
      "epoch: 1, loss: 0.045334, val_acc:0.68, test_acc:0.7\n",
      "epoch: 2, loss: 0.04438, val_acc:0.52, test_acc:0.58\n",
      "epoch: 3, loss: 0.044594, val_acc:0.6, test_acc:0.68\n",
      "epoch: 4, loss: 0.043104, val_acc:0.63, test_acc:0.7\n",
      "epoch: 5, loss: 0.04313, val_acc:0.68, test_acc:0.71\n",
      "epoch: 6, loss: 0.041492, val_acc:0.69, test_acc:0.74\n",
      "epoch: 7, loss: 0.040916, val_acc:0.65, test_acc:0.7\n",
      "epoch: 8, loss: 0.041496, val_acc:0.67, test_acc:0.73\n",
      "epoch: 9, loss: 0.041105, val_acc:0.7, test_acc:0.75\n",
      "epoch: 10, loss: 0.040787, val_acc:0.65, test_acc:0.71\n",
      "epoch: 11, loss: 0.040085, val_acc:0.69, test_acc:0.75\n",
      "epoch: 12, loss: 0.039842, val_acc:0.64, test_acc:0.7\n",
      "epoch: 13, loss: 0.039249, val_acc:0.68, test_acc:0.76\n",
      "epoch: 14, loss: 0.039094, val_acc:0.76, test_acc:0.73\n",
      "epoch: 15, loss: 0.038291, val_acc:0.71, test_acc:0.72\n",
      "epoch: 16, loss: 0.0378, val_acc:0.81, test_acc:0.8\n",
      "epoch: 17, loss: 0.039242, val_acc:0.83, test_acc:0.84\n",
      "epoch: 18, loss: 0.03818, val_acc:0.82, test_acc:0.79\n",
      "epoch: 19, loss: 0.038127, val_acc:0.67, test_acc:0.74\n",
      "epoch: 20, loss: 0.036951, val_acc:0.72, test_acc:0.77\n",
      "epoch: 21, loss: 0.037113, val_acc:0.78, test_acc:0.81\n",
      "epoch: 22, loss: 0.036826, val_acc:0.75, test_acc:0.8\n",
      "epoch: 23, loss: 0.03663, val_acc:0.69, test_acc:0.73\n",
      "epoch: 24, loss: 0.036829, val_acc:0.8, test_acc:0.81\n",
      "epoch: 25, loss: 0.036412, val_acc:0.79, test_acc:0.79\n",
      "epoch: 26, loss: 0.036572, val_acc:0.78, test_acc:0.82\n",
      "epoch: 27, loss: 0.035267, val_acc:0.75, test_acc:0.83\n",
      "epoch: 28, loss: 0.03673, val_acc:0.69, test_acc:0.76\n",
      "epoch: 29, loss: 0.035942, val_acc:0.78, test_acc:0.79\n",
      "epoch: 30, loss: 0.036441, val_acc:0.79, test_acc:0.81\n",
      "epoch: 31, loss: 0.034921, val_acc:0.7, test_acc:0.73\n",
      "epoch: 32, loss: 0.036609, val_acc:0.77, test_acc:0.81\n",
      "epoch: 33, loss: 0.034869, val_acc:0.77, test_acc:0.79\n",
      "epoch: 34, loss: 0.034643, val_acc:0.71, test_acc:0.75\n",
      "epoch: 35, loss: 0.033628, val_acc:0.8, test_acc:0.81\n",
      "epoch: 36, loss: 0.035221, val_acc:0.8, test_acc:0.79\n",
      "epoch: 37, loss: 0.033696, val_acc:0.74, test_acc:0.8\n",
      "epoch: 38, loss: 0.033582, val_acc:0.85, test_acc:0.82\n",
      "epoch: 39, loss: 0.033493, val_acc:0.81, test_acc:0.83\n",
      "epoch: 40, loss: 0.033682, val_acc:0.78, test_acc:0.83\n",
      "epoch: 41, loss: 0.032491, val_acc:0.85, test_acc:0.81\n",
      "epoch: 42, loss: 0.033742, val_acc:0.86, test_acc:0.84\n",
      "epoch: 43, loss: 0.033105, val_acc:0.81, test_acc:0.84\n",
      "epoch: 44, loss: 0.034221, val_acc:0.88, test_acc:0.88\n",
      "epoch: 45, loss: 0.034564, val_acc:0.84, test_acc:0.85\n",
      "epoch: 46, loss: 0.033272, val_acc:0.82, test_acc:0.81\n",
      "epoch: 47, loss: 0.033765, val_acc:0.81, test_acc:0.8\n",
      "epoch: 48, loss: 0.034132, val_acc:0.82, test_acc:0.86\n",
      "epoch: 49, loss: 0.033113, val_acc:0.8, test_acc:0.83\n",
      "epoch: 50, loss: 0.03147, val_acc:0.81, test_acc:0.83\n",
      "epoch: 51, loss: 0.033095, val_acc:0.82, test_acc:0.85\n",
      "epoch: 52, loss: 0.031063, val_acc:0.83, test_acc:0.85\n",
      "epoch: 53, loss: 0.031372, val_acc:0.81, test_acc:0.82\n",
      "epoch: 54, loss: 0.031904, val_acc:0.84, test_acc:0.86\n",
      "epoch: 55, loss: 0.032065, val_acc:0.81, test_acc:0.81\n",
      "epoch: 56, loss: 0.03109, val_acc:0.81, test_acc:0.87\n",
      "epoch: 57, loss: 0.031882, val_acc:0.85, test_acc:0.85\n",
      "epoch: 58, loss: 0.031936, val_acc:0.84, test_acc:0.84\n",
      "epoch: 59, loss: 0.032666, val_acc:0.84, test_acc:0.83\n",
      "epoch: 60, loss: 0.03032, val_acc:0.86, test_acc:0.86\n",
      "epoch: 61, loss: 0.031354, val_acc:0.77, test_acc:0.82\n",
      "epoch: 62, loss: 0.030827, val_acc:0.78, test_acc:0.83\n",
      "epoch: 63, loss: 0.031107, val_acc:0.87, test_acc:0.86\n",
      "epoch: 64, loss: 0.03098, val_acc:0.87, test_acc:0.89\n",
      "epoch: 65, loss: 0.031247, val_acc:0.8, test_acc:0.84\n",
      "epoch: 66, loss: 0.030714, val_acc:0.84, test_acc:0.85\n",
      "epoch: 67, loss: 0.030776, val_acc:0.85, test_acc:0.87\n",
      "epoch: 68, loss: 0.031679, val_acc:0.8, test_acc:0.88\n",
      "epoch: 69, loss: 0.029816, val_acc:0.8, test_acc:0.84\n",
      "epoch: 70, loss: 0.030224, val_acc:0.84, test_acc:0.88\n",
      "epoch: 71, loss: 0.029766, val_acc:0.86, test_acc:0.88\n",
      "epoch: 72, loss: 0.029099, val_acc:0.77, test_acc:0.83\n",
      "epoch: 73, loss: 0.029177, val_acc:0.81, test_acc:0.87\n",
      "epoch: 74, loss: 0.029789, val_acc:0.83, test_acc:0.87\n",
      "epoch: 75, loss: 0.029936, val_acc:0.82, test_acc:0.86\n",
      "epoch: 76, loss: 0.02965, val_acc:0.82, test_acc:0.88\n",
      "epoch: 77, loss: 0.028789, val_acc:0.79, test_acc:0.86\n",
      "epoch: 78, loss: 0.02848, val_acc:0.83, test_acc:0.87\n",
      "epoch: 79, loss: 0.028998, val_acc:0.82, test_acc:0.85\n",
      "epoch: 80, loss: 0.028844, val_acc:0.86, test_acc:0.85\n",
      "epoch: 81, loss: 0.028314, val_acc:0.86, test_acc:0.88\n",
      "epoch: 82, loss: 0.029058, val_acc:0.86, test_acc:0.87\n",
      "epoch: 83, loss: 0.028929, val_acc:0.85, test_acc:0.89\n",
      "epoch: 84, loss: 0.027313, val_acc:0.87, test_acc:0.88\n",
      "epoch: 85, loss: 0.027517, val_acc:0.8, test_acc:0.85\n",
      "epoch: 86, loss: 0.028455, val_acc:0.83, test_acc:0.85\n",
      "epoch: 87, loss: 0.027746, val_acc:0.92, test_acc:0.89\n",
      "epoch: 88, loss: 0.027469, val_acc:0.84, test_acc:0.85\n",
      "epoch: 89, loss: 0.028883, val_acc:0.85, test_acc:0.87\n",
      "epoch: 90, loss: 0.027792, val_acc:0.86, test_acc:0.88\n",
      "epoch: 91, loss: 0.028057, val_acc:0.87, test_acc:0.87\n",
      "epoch: 92, loss: 0.026338, val_acc:0.91, test_acc:0.88\n",
      "epoch: 93, loss: 0.027957, val_acc:0.88, test_acc:0.89\n",
      "epoch: 94, loss: 0.028077, val_acc:0.86, test_acc:0.87\n",
      "epoch: 95, loss: 0.026979, val_acc:0.86, test_acc:0.87\n",
      "epoch: 96, loss: 0.026614, val_acc:0.86, test_acc:0.88\n",
      "epoch: 97, loss: 0.026577, val_acc:0.88, test_acc:0.91\n",
      "epoch: 98, loss: 0.026674, val_acc:0.87, test_acc:0.88\n",
      "epoch: 99, loss: 0.025632, val_acc:0.88, test_acc:0.87\n"
     ]
    }
   ],
   "source": [
    "#model=ResidualAttentionalGNN1(args=args, train_dataset=train_dataset, hidden_channels=args.hidden, hidden=args.hidden_mlp, num_layers=args.num_layers, GNN=gnn, alpha=0.5).to(args.device)\n",
    "%run main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2558f070-d131-4488-ac2a-25da3942138f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1078\n",
      "dataset loaded successfully! HCPGender\n",
      "dataset HCPGender loaded with train 754 val 108 test 216 splits\n",
      "1000\n",
      "3\n",
      "ResidualAttentionalGNN1(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1000, 32)\n",
      "    (1-2): 2 x GCNConv(32, 32)\n",
      "  )\n",
      "  (attention_layers): ModuleList(\n",
      "    (0-2): 3 x Attention1(\n",
      "      (linear): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (aggr): MeanAggregation()\n",
      "  (bn): BatchNorm1d(998504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=998600, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters is: 65948434\n",
      "epoch: 0, loss: 0.047863, val_acc:0.5, test_acc:0.58\n",
      "epoch: 1, loss: 0.045416, val_acc:0.64, test_acc:0.69\n",
      "epoch: 2, loss: 0.043569, val_acc:0.63, test_acc:0.65\n",
      "epoch: 3, loss: 0.044073, val_acc:0.56, test_acc:0.59\n",
      "epoch: 4, loss: 0.04356, val_acc:0.58, test_acc:0.65\n",
      "epoch: 5, loss: 0.043724, val_acc:0.66, test_acc:0.64\n",
      "epoch: 6, loss: 0.042202, val_acc:0.69, test_acc:0.75\n",
      "epoch: 7, loss: 0.040704, val_acc:0.66, test_acc:0.68\n",
      "epoch: 8, loss: 0.041361, val_acc:0.67, test_acc:0.76\n",
      "epoch: 9, loss: 0.039924, val_acc:0.72, test_acc:0.71\n",
      "epoch: 10, loss: 0.04108, val_acc:0.78, test_acc:0.75\n",
      "epoch: 11, loss: 0.040199, val_acc:0.83, test_acc:0.81\n",
      "epoch: 12, loss: 0.038709, val_acc:0.82, test_acc:0.82\n",
      "epoch: 13, loss: 0.040479, val_acc:0.77, test_acc:0.78\n",
      "epoch: 14, loss: 0.03852, val_acc:0.77, test_acc:0.79\n",
      "epoch: 15, loss: 0.03753, val_acc:0.81, test_acc:0.76\n",
      "epoch: 16, loss: 0.037195, val_acc:0.81, test_acc:0.82\n",
      "epoch: 17, loss: 0.038991, val_acc:0.78, test_acc:0.78\n",
      "epoch: 18, loss: 0.037141, val_acc:0.75, test_acc:0.77\n",
      "epoch: 19, loss: 0.038069, val_acc:0.8, test_acc:0.79\n",
      "epoch: 20, loss: 0.03639, val_acc:0.83, test_acc:0.81\n",
      "epoch: 21, loss: 0.036984, val_acc:0.8, test_acc:0.8\n",
      "epoch: 22, loss: 0.036769, val_acc:0.82, test_acc:0.84\n",
      "epoch: 23, loss: 0.036468, val_acc:0.81, test_acc:0.78\n",
      "epoch: 24, loss: 0.036072, val_acc:0.83, test_acc:0.83\n",
      "epoch: 25, loss: 0.03567, val_acc:0.8, test_acc:0.78\n",
      "epoch: 26, loss: 0.035271, val_acc:0.79, test_acc:0.8\n",
      "epoch: 27, loss: 0.034668, val_acc:0.69, test_acc:0.79\n",
      "epoch: 28, loss: 0.03674, val_acc:0.75, test_acc:0.79\n",
      "epoch: 29, loss: 0.035808, val_acc:0.81, test_acc:0.82\n",
      "epoch: 30, loss: 0.035866, val_acc:0.81, test_acc:0.8\n",
      "epoch: 31, loss: 0.034859, val_acc:0.75, test_acc:0.79\n",
      "epoch: 32, loss: 0.036741, val_acc:0.81, test_acc:0.82\n",
      "epoch: 33, loss: 0.034721, val_acc:0.78, test_acc:0.82\n",
      "epoch: 34, loss: 0.034145, val_acc:0.81, test_acc:0.82\n",
      "epoch: 35, loss: 0.033622, val_acc:0.85, test_acc:0.84\n",
      "epoch: 36, loss: 0.03539, val_acc:0.84, test_acc:0.81\n",
      "epoch: 37, loss: 0.03325, val_acc:0.81, test_acc:0.83\n",
      "epoch: 38, loss: 0.03319, val_acc:0.78, test_acc:0.83\n",
      "epoch: 39, loss: 0.03415, val_acc:0.83, test_acc:0.83\n",
      "epoch: 40, loss: 0.033894, val_acc:0.82, test_acc:0.84\n",
      "epoch: 41, loss: 0.033591, val_acc:0.76, test_acc:0.83\n",
      "epoch: 42, loss: 0.032915, val_acc:0.78, test_acc:0.81\n",
      "epoch: 43, loss: 0.033359, val_acc:0.75, test_acc:0.81\n",
      "epoch: 44, loss: 0.032932, val_acc:0.83, test_acc:0.84\n",
      "epoch: 45, loss: 0.033747, val_acc:0.89, test_acc:0.87\n",
      "epoch: 46, loss: 0.033652, val_acc:0.86, test_acc:0.86\n",
      "epoch: 47, loss: 0.03315, val_acc:0.82, test_acc:0.85\n",
      "epoch: 48, loss: 0.03445, val_acc:0.87, test_acc:0.87\n",
      "epoch: 49, loss: 0.032136, val_acc:0.87, test_acc:0.86\n",
      "epoch: 50, loss: 0.031452, val_acc:0.81, test_acc:0.85\n",
      "epoch: 51, loss: 0.032064, val_acc:0.85, test_acc:0.85\n",
      "epoch: 52, loss: 0.030993, val_acc:0.82, test_acc:0.84\n",
      "epoch: 53, loss: 0.031136, val_acc:0.84, test_acc:0.81\n",
      "epoch: 54, loss: 0.031299, val_acc:0.88, test_acc:0.81\n",
      "epoch: 55, loss: 0.032065, val_acc:0.88, test_acc:0.83\n",
      "epoch: 56, loss: 0.031405, val_acc:0.88, test_acc:0.86\n",
      "epoch: 57, loss: 0.032467, val_acc:0.87, test_acc:0.85\n",
      "epoch: 58, loss: 0.031174, val_acc:0.83, test_acc:0.84\n",
      "epoch: 59, loss: 0.031518, val_acc:0.85, test_acc:0.86\n",
      "epoch: 60, loss: 0.029903, val_acc:0.87, test_acc:0.88\n",
      "epoch: 61, loss: 0.030772, val_acc:0.82, test_acc:0.85\n",
      "epoch: 62, loss: 0.030945, val_acc:0.84, test_acc:0.83\n",
      "epoch: 63, loss: 0.030713, val_acc:0.88, test_acc:0.86\n",
      "epoch: 64, loss: 0.030279, val_acc:0.84, test_acc:0.87\n",
      "epoch: 65, loss: 0.030802, val_acc:0.88, test_acc:0.86\n",
      "epoch: 66, loss: 0.029502, val_acc:0.85, test_acc:0.84\n",
      "epoch: 67, loss: 0.029982, val_acc:0.83, test_acc:0.86\n",
      "epoch: 68, loss: 0.029846, val_acc:0.86, test_acc:0.87\n",
      "epoch: 69, loss: 0.030347, val_acc:0.84, test_acc:0.85\n",
      "epoch: 70, loss: 0.029896, val_acc:0.88, test_acc:0.88\n",
      "epoch: 71, loss: 0.029476, val_acc:0.83, test_acc:0.87\n",
      "epoch: 72, loss: 0.028969, val_acc:0.88, test_acc:0.88\n",
      "epoch: 73, loss: 0.02871, val_acc:0.89, test_acc:0.89\n",
      "epoch: 74, loss: 0.028515, val_acc:0.87, test_acc:0.88\n",
      "epoch: 75, loss: 0.030091, val_acc:0.86, test_acc:0.88\n",
      "epoch: 76, loss: 0.029514, val_acc:0.89, test_acc:0.87\n",
      "epoch: 77, loss: 0.028991, val_acc:0.84, test_acc:0.86\n",
      "epoch: 78, loss: 0.028976, val_acc:0.86, test_acc:0.86\n",
      "epoch: 79, loss: 0.029096, val_acc:0.86, test_acc:0.83\n",
      "epoch: 80, loss: 0.028433, val_acc:0.83, test_acc:0.85\n",
      "epoch: 81, loss: 0.027888, val_acc:0.85, test_acc:0.85\n",
      "epoch: 82, loss: 0.028714, val_acc:0.86, test_acc:0.88\n",
      "epoch: 83, loss: 0.027811, val_acc:0.86, test_acc:0.87\n",
      "epoch: 84, loss: 0.027063, val_acc:0.85, test_acc:0.86\n",
      "epoch: 85, loss: 0.027614, val_acc:0.85, test_acc:0.85\n",
      "epoch: 86, loss: 0.027691, val_acc:0.86, test_acc:0.86\n",
      "epoch: 87, loss: 0.027088, val_acc:0.89, test_acc:0.88\n",
      "epoch: 88, loss: 0.027989, val_acc:0.85, test_acc:0.85\n",
      "epoch: 89, loss: 0.027508, val_acc:0.9, test_acc:0.88\n",
      "epoch: 90, loss: 0.026883, val_acc:0.87, test_acc:0.85\n",
      "epoch: 91, loss: 0.027331, val_acc:0.86, test_acc:0.87\n",
      "epoch: 92, loss: 0.026509, val_acc:0.83, test_acc:0.86\n",
      "epoch: 93, loss: 0.027523, val_acc:0.89, test_acc:0.89\n",
      "epoch: 94, loss: 0.027223, val_acc:0.81, test_acc:0.84\n",
      "epoch: 95, loss: 0.026605, val_acc:0.87, test_acc:0.89\n",
      "epoch: 96, loss: 0.027039, val_acc:0.86, test_acc:0.87\n",
      "epoch: 97, loss: 0.025468, val_acc:0.89, test_acc:0.88\n",
      "epoch: 98, loss: 0.026309, val_acc:0.85, test_acc:0.87\n",
      "epoch: 99, loss: 0.026029, val_acc:0.85, test_acc:0.86\n"
     ]
    }
   ],
   "source": [
    "#model=ResidualAttentionalGNN1\n",
    "%run main --alpha 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a99fa56-8d36-4d47-8c09-3c279efbd23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1078\n",
      "dataset loaded successfully! HCPGender\n",
      "dataset HCPGender loaded with train 754 val 108 test 216 splits\n",
      "1000\n",
      "3\n",
      "ResidualAttentionalGNN1(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1000, 32)\n",
      "    (1-2): 2 x GCNConv(32, 32)\n",
      "  )\n",
      "  (attention_layers): ModuleList(\n",
      "    (0-2): 3 x Attention1(\n",
      "      (linear): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (aggr): MeanAggregation()\n",
      "  (bn): BatchNorm1d(998504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=998600, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters is: 65948434\n",
      "epoch: 0, loss: 0.047848, val_acc:0.54, test_acc:0.61\n",
      "epoch: 1, loss: 0.04572, val_acc:0.68, test_acc:0.68\n",
      "epoch: 2, loss: 0.043445, val_acc:0.54, test_acc:0.66\n",
      "epoch: 3, loss: 0.043523, val_acc:0.56, test_acc:0.58\n",
      "epoch: 4, loss: 0.04381, val_acc:0.56, test_acc:0.6\n",
      "epoch: 5, loss: 0.043003, val_acc:0.56, test_acc:0.58\n",
      "epoch: 6, loss: 0.042182, val_acc:0.66, test_acc:0.74\n",
      "epoch: 7, loss: 0.041486, val_acc:0.67, test_acc:0.69\n",
      "epoch: 8, loss: 0.041588, val_acc:0.71, test_acc:0.72\n",
      "epoch: 9, loss: 0.040634, val_acc:0.75, test_acc:0.73\n",
      "epoch: 10, loss: 0.040969, val_acc:0.71, test_acc:0.76\n",
      "epoch: 11, loss: 0.040453, val_acc:0.78, test_acc:0.8\n",
      "epoch: 12, loss: 0.03913, val_acc:0.77, test_acc:0.78\n",
      "epoch: 13, loss: 0.039179, val_acc:0.73, test_acc:0.77\n",
      "epoch: 14, loss: 0.038761, val_acc:0.79, test_acc:0.8\n",
      "epoch: 15, loss: 0.038361, val_acc:0.79, test_acc:0.8\n",
      "epoch: 16, loss: 0.038745, val_acc:0.77, test_acc:0.75\n",
      "epoch: 17, loss: 0.03928, val_acc:0.83, test_acc:0.79\n",
      "epoch: 18, loss: 0.036786, val_acc:0.77, test_acc:0.75\n",
      "epoch: 19, loss: 0.03819, val_acc:0.74, test_acc:0.8\n",
      "epoch: 20, loss: 0.036431, val_acc:0.81, test_acc:0.81\n",
      "epoch: 21, loss: 0.038004, val_acc:0.83, test_acc:0.79\n",
      "epoch: 22, loss: 0.036415, val_acc:0.79, test_acc:0.8\n",
      "epoch: 23, loss: 0.037098, val_acc:0.81, test_acc:0.85\n",
      "epoch: 24, loss: 0.035727, val_acc:0.81, test_acc:0.78\n",
      "epoch: 25, loss: 0.036806, val_acc:0.79, test_acc:0.82\n",
      "epoch: 26, loss: 0.035497, val_acc:0.78, test_acc:0.81\n",
      "epoch: 27, loss: 0.03525, val_acc:0.81, test_acc:0.83\n",
      "epoch: 28, loss: 0.03568, val_acc:0.81, test_acc:0.78\n",
      "epoch: 29, loss: 0.035685, val_acc:0.76, test_acc:0.84\n",
      "epoch: 30, loss: 0.036686, val_acc:0.77, test_acc:0.82\n",
      "epoch: 31, loss: 0.03527, val_acc:0.74, test_acc:0.82\n",
      "epoch: 32, loss: 0.035586, val_acc:0.79, test_acc:0.85\n",
      "epoch: 33, loss: 0.034621, val_acc:0.81, test_acc:0.83\n",
      "epoch: 34, loss: 0.036069, val_acc:0.78, test_acc:0.81\n",
      "epoch: 35, loss: 0.034879, val_acc:0.81, test_acc:0.82\n",
      "epoch: 36, loss: 0.036028, val_acc:0.76, test_acc:0.8\n",
      "epoch: 37, loss: 0.034044, val_acc:0.81, test_acc:0.81\n",
      "epoch: 38, loss: 0.033312, val_acc:0.82, test_acc:0.82\n",
      "epoch: 39, loss: 0.033928, val_acc:0.77, test_acc:0.78\n",
      "epoch: 40, loss: 0.034296, val_acc:0.83, test_acc:0.85\n",
      "epoch: 41, loss: 0.033081, val_acc:0.85, test_acc:0.82\n",
      "epoch: 42, loss: 0.034141, val_acc:0.87, test_acc:0.84\n",
      "epoch: 43, loss: 0.033443, val_acc:0.81, test_acc:0.83\n",
      "epoch: 44, loss: 0.033296, val_acc:0.87, test_acc:0.79\n",
      "epoch: 45, loss: 0.034356, val_acc:0.85, test_acc:0.79\n",
      "epoch: 46, loss: 0.032557, val_acc:0.82, test_acc:0.85\n",
      "epoch: 47, loss: 0.032501, val_acc:0.78, test_acc:0.81\n",
      "epoch: 48, loss: 0.033875, val_acc:0.82, test_acc:0.81\n",
      "epoch: 49, loss: 0.032572, val_acc:0.87, test_acc:0.83\n",
      "epoch: 50, loss: 0.031236, val_acc:0.86, test_acc:0.83\n",
      "epoch: 51, loss: 0.032898, val_acc:0.82, test_acc:0.84\n",
      "epoch: 52, loss: 0.031408, val_acc:0.83, test_acc:0.86\n",
      "epoch: 53, loss: 0.031895, val_acc:0.78, test_acc:0.79\n",
      "epoch: 54, loss: 0.032387, val_acc:0.85, test_acc:0.85\n",
      "epoch: 55, loss: 0.031583, val_acc:0.82, test_acc:0.84\n",
      "epoch: 56, loss: 0.030892, val_acc:0.84, test_acc:0.85\n",
      "epoch: 57, loss: 0.032805, val_acc:0.85, test_acc:0.83\n",
      "epoch: 58, loss: 0.031984, val_acc:0.84, test_acc:0.85\n",
      "epoch: 59, loss: 0.031497, val_acc:0.89, test_acc:0.86\n",
      "epoch: 60, loss: 0.03035, val_acc:0.87, test_acc:0.85\n",
      "epoch: 61, loss: 0.030578, val_acc:0.86, test_acc:0.86\n",
      "epoch: 62, loss: 0.031, val_acc:0.81, test_acc:0.87\n",
      "epoch: 63, loss: 0.031177, val_acc:0.84, test_acc:0.86\n",
      "epoch: 64, loss: 0.030235, val_acc:0.87, test_acc:0.84\n",
      "epoch: 65, loss: 0.031884, val_acc:0.81, test_acc:0.86\n",
      "epoch: 66, loss: 0.031249, val_acc:0.81, test_acc:0.85\n",
      "epoch: 67, loss: 0.03136, val_acc:0.89, test_acc:0.88\n",
      "epoch: 68, loss: 0.030574, val_acc:0.81, test_acc:0.86\n",
      "epoch: 69, loss: 0.030411, val_acc:0.83, test_acc:0.84\n",
      "epoch: 70, loss: 0.030669, val_acc:0.86, test_acc:0.86\n",
      "epoch: 71, loss: 0.030087, val_acc:0.86, test_acc:0.88\n",
      "epoch: 72, loss: 0.029819, val_acc:0.81, test_acc:0.87\n",
      "epoch: 73, loss: 0.030002, val_acc:0.86, test_acc:0.87\n",
      "epoch: 74, loss: 0.028869, val_acc:0.81, test_acc:0.85\n",
      "epoch: 75, loss: 0.030507, val_acc:0.84, test_acc:0.87\n",
      "epoch: 76, loss: 0.03074, val_acc:0.81, test_acc:0.84\n",
      "epoch: 77, loss: 0.028426, val_acc:0.88, test_acc:0.86\n",
      "epoch: 78, loss: 0.02994, val_acc:0.84, test_acc:0.87\n",
      "epoch: 79, loss: 0.028873, val_acc:0.85, test_acc:0.85\n",
      "epoch: 80, loss: 0.0281, val_acc:0.88, test_acc:0.88\n",
      "epoch: 81, loss: 0.028907, val_acc:0.87, test_acc:0.88\n",
      "epoch: 82, loss: 0.028919, val_acc:0.89, test_acc:0.88\n",
      "epoch: 83, loss: 0.028297, val_acc:0.86, test_acc:0.88\n",
      "epoch: 84, loss: 0.027148, val_acc:0.9, test_acc:0.88\n",
      "epoch: 85, loss: 0.026563, val_acc:0.87, test_acc:0.86\n",
      "epoch: 86, loss: 0.027972, val_acc:0.86, test_acc:0.87\n",
      "epoch: 87, loss: 0.027827, val_acc:0.84, test_acc:0.86\n",
      "epoch: 88, loss: 0.02723, val_acc:0.83, test_acc:0.86\n",
      "epoch: 89, loss: 0.028444, val_acc:0.85, test_acc:0.89\n",
      "epoch: 90, loss: 0.026674, val_acc:0.85, test_acc:0.88\n",
      "epoch: 91, loss: 0.027498, val_acc:0.84, test_acc:0.88\n",
      "epoch: 92, loss: 0.026657, val_acc:0.85, test_acc:0.89\n",
      "epoch: 93, loss: 0.02865, val_acc:0.87, test_acc:0.89\n",
      "epoch: 94, loss: 0.027629, val_acc:0.87, test_acc:0.88\n",
      "epoch: 95, loss: 0.026946, val_acc:0.86, test_acc:0.87\n",
      "epoch: 96, loss: 0.026548, val_acc:0.84, test_acc:0.89\n",
      "epoch: 97, loss: 0.027072, val_acc:0.84, test_acc:0.88\n",
      "epoch: 98, loss: 0.027212, val_acc:0.84, test_acc:0.89\n",
      "epoch: 99, loss: 0.025699, val_acc:0.87, test_acc:0.87\n"
     ]
    }
   ],
   "source": [
    "#model=ResidualAttentionalGNN1\n",
    "%run main --alpha 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611803ce-4ace-4bcd-901d-d5b77a7125fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1078\n",
      "dataset loaded successfully! HCPGender\n",
      "dataset HCPGender loaded with train 754 val 108 test 216 splits\n",
      "ResidualAttentionalGNN3(\n",
      "  (aggr): MeanAggregation()\n",
      "  (convs): ModuleList(\n",
      "    (0-2): 3 x GNNWithAttention()\n",
      "  )\n",
      "  (bn): BatchNorm1d(998504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=998600, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters is: 65945458\n",
      "epoch: 0, loss: 0.048225, val_acc:0.53, test_acc:0.56\n",
      "epoch: 1, loss: 0.046868, val_acc:0.52, test_acc:0.62\n",
      "epoch: 2, loss: 0.044085, val_acc:0.64, test_acc:0.67\n",
      "epoch: 3, loss: 0.041019, val_acc:0.6, test_acc:0.68\n",
      "epoch: 4, loss: 0.041976, val_acc:0.65, test_acc:0.73\n",
      "epoch: 5, loss: 0.042623, val_acc:0.55, test_acc:0.61\n",
      "epoch: 6, loss: 0.041266, val_acc:0.66, test_acc:0.67\n",
      "epoch: 7, loss: 0.040272, val_acc:0.68, test_acc:0.71\n",
      "epoch: 8, loss: 0.039348, val_acc:0.76, test_acc:0.78\n",
      "epoch: 9, loss: 0.038049, val_acc:0.71, test_acc:0.74\n",
      "epoch: 10, loss: 0.038918, val_acc:0.65, test_acc:0.69\n",
      "epoch: 11, loss: 0.036285, val_acc:0.63, test_acc:0.7\n",
      "epoch: 12, loss: 0.036623, val_acc:0.74, test_acc:0.73\n",
      "epoch: 13, loss: 0.035485, val_acc:0.69, test_acc:0.74\n",
      "epoch: 14, loss: 0.037004, val_acc:0.71, test_acc:0.73\n",
      "epoch: 15, loss: 0.035486, val_acc:0.69, test_acc:0.73\n",
      "epoch: 16, loss: 0.035817, val_acc:0.7, test_acc:0.71\n",
      "epoch: 17, loss: 0.034634, val_acc:0.74, test_acc:0.72\n",
      "epoch: 18, loss: 0.03573, val_acc:0.75, test_acc:0.76\n",
      "epoch: 19, loss: 0.033689, val_acc:0.69, test_acc:0.71\n",
      "epoch: 20, loss: 0.035879, val_acc:0.73, test_acc:0.75\n",
      "epoch: 21, loss: 0.03482, val_acc:0.72, test_acc:0.75\n",
      "epoch: 22, loss: 0.03354, val_acc:0.71, test_acc:0.75\n",
      "epoch: 23, loss: 0.03362, val_acc:0.71, test_acc:0.74\n",
      "epoch: 24, loss: 0.033456, val_acc:0.77, test_acc:0.79\n",
      "epoch: 25, loss: 0.033288, val_acc:0.77, test_acc:0.77\n",
      "epoch: 26, loss: 0.032592, val_acc:0.7, test_acc:0.71\n",
      "epoch: 27, loss: 0.032555, val_acc:0.78, test_acc:0.78\n",
      "epoch: 28, loss: 0.031912, val_acc:0.74, test_acc:0.76\n",
      "epoch: 29, loss: 0.032597, val_acc:0.73, test_acc:0.75\n",
      "epoch: 30, loss: 0.03155, val_acc:0.71, test_acc:0.74\n",
      "epoch: 31, loss: 0.030689, val_acc:0.71, test_acc:0.74\n",
      "epoch: 32, loss: 0.031519, val_acc:0.76, test_acc:0.76\n",
      "epoch: 33, loss: 0.031675, val_acc:0.72, test_acc:0.75\n",
      "epoch: 34, loss: 0.030791, val_acc:0.67, test_acc:0.69\n",
      "epoch: 35, loss: 0.032032, val_acc:0.65, test_acc:0.68\n",
      "epoch: 36, loss: 0.032172, val_acc:0.74, test_acc:0.74\n",
      "epoch: 37, loss: 0.030439, val_acc:0.63, test_acc:0.69\n",
      "epoch: 38, loss: 0.031553, val_acc:0.75, test_acc:0.75\n",
      "epoch: 39, loss: 0.029991, val_acc:0.77, test_acc:0.75\n",
      "epoch: 40, loss: 0.030681, val_acc:0.69, test_acc:0.72\n",
      "epoch: 41, loss: 0.030357, val_acc:0.77, test_acc:0.76\n",
      "epoch: 42, loss: 0.030865, val_acc:0.73, test_acc:0.75\n",
      "epoch: 43, loss: 0.029997, val_acc:0.79, test_acc:0.81\n",
      "epoch: 44, loss: 0.030536, val_acc:0.77, test_acc:0.79\n",
      "epoch: 45, loss: 0.029801, val_acc:0.75, test_acc:0.75\n",
      "epoch: 46, loss: 0.029816, val_acc:0.81, test_acc:0.8\n",
      "epoch: 47, loss: 0.028361, val_acc:0.74, test_acc:0.73\n",
      "epoch: 48, loss: 0.02992, val_acc:0.73, test_acc:0.74\n",
      "epoch: 49, loss: 0.030085, val_acc:0.77, test_acc:0.74\n",
      "epoch: 50, loss: 0.028482, val_acc:0.69, test_acc:0.73\n",
      "epoch: 51, loss: 0.027933, val_acc:0.78, test_acc:0.76\n",
      "epoch: 52, loss: 0.028937, val_acc:0.83, test_acc:0.81\n",
      "epoch: 53, loss: 0.029175, val_acc:0.77, test_acc:0.75\n",
      "epoch: 54, loss: 0.028476, val_acc:0.81, test_acc:0.76\n",
      "epoch: 55, loss: 0.02846, val_acc:0.78, test_acc:0.77\n",
      "epoch: 56, loss: 0.028098, val_acc:0.74, test_acc:0.73\n",
      "epoch: 57, loss: 0.028908, val_acc:0.72, test_acc:0.73\n",
      "epoch: 58, loss: 0.028668, val_acc:0.7, test_acc:0.72\n",
      "epoch: 59, loss: 0.027241, val_acc:0.69, test_acc:0.71\n",
      "epoch: 60, loss: 0.028567, val_acc:0.77, test_acc:0.75\n",
      "epoch: 61, loss: 0.027919, val_acc:0.72, test_acc:0.71\n",
      "epoch: 62, loss: 0.027333, val_acc:0.81, test_acc:0.78\n",
      "epoch: 63, loss: 0.026961, val_acc:0.75, test_acc:0.75\n",
      "epoch: 64, loss: 0.027054, val_acc:0.69, test_acc:0.72\n",
      "epoch: 65, loss: 0.027573, val_acc:0.76, test_acc:0.74\n",
      "epoch: 66, loss: 0.026097, val_acc:0.69, test_acc:0.71\n",
      "epoch: 67, loss: 0.026917, val_acc:0.76, test_acc:0.79\n",
      "epoch: 68, loss: 0.027377, val_acc:0.75, test_acc:0.76\n",
      "epoch: 69, loss: 0.026379, val_acc:0.78, test_acc:0.75\n",
      "epoch: 70, loss: 0.026376, val_acc:0.75, test_acc:0.77\n",
      "epoch: 71, loss: 0.027167, val_acc:0.78, test_acc:0.75\n",
      "epoch: 72, loss: 0.026992, val_acc:0.81, test_acc:0.78\n",
      "epoch: 73, loss: 0.025672, val_acc:0.75, test_acc:0.74\n",
      "epoch: 74, loss: 0.026544, val_acc:0.77, test_acc:0.78\n",
      "epoch: 75, loss: 0.025908, val_acc:0.84, test_acc:0.8\n",
      "epoch: 76, loss: 0.026167, val_acc:0.78, test_acc:0.78\n",
      "epoch: 77, loss: 0.025564, val_acc:0.79, test_acc:0.77\n",
      "epoch: 78, loss: 0.0274, val_acc:0.71, test_acc:0.71\n",
      "epoch: 79, loss: 0.027052, val_acc:0.76, test_acc:0.74\n",
      "epoch: 80, loss: 0.026768, val_acc:0.73, test_acc:0.74\n",
      "epoch: 81, loss: 0.025527, val_acc:0.69, test_acc:0.73\n",
      "epoch: 82, loss: 0.025783, val_acc:0.74, test_acc:0.75\n",
      "epoch: 83, loss: 0.024748, val_acc:0.82, test_acc:0.8\n",
      "epoch: 84, loss: 0.025239, val_acc:0.78, test_acc:0.75\n",
      "epoch: 85, loss: 0.025372, val_acc:0.73, test_acc:0.74\n",
      "epoch: 86, loss: 0.024646, val_acc:0.73, test_acc:0.75\n",
      "epoch: 87, loss: 0.024624, val_acc:0.75, test_acc:0.78\n",
      "epoch: 88, loss: 0.025201, val_acc:0.76, test_acc:0.75\n",
      "epoch: 89, loss: 0.024775, val_acc:0.73, test_acc:0.75\n",
      "epoch: 90, loss: 0.02527, val_acc:0.74, test_acc:0.76\n",
      "epoch: 91, loss: 0.025085, val_acc:0.74, test_acc:0.75\n",
      "epoch: 92, loss: 0.025061, val_acc:0.8, test_acc:0.78\n",
      "epoch: 93, loss: 0.023957, val_acc:0.77, test_acc:0.76\n",
      "epoch: 94, loss: 0.023595, val_acc:0.8, test_acc:0.79\n",
      "epoch: 95, loss: 0.023276, val_acc:0.75, test_acc:0.74\n",
      "epoch: 96, loss: 0.024697, val_acc:0.76, test_acc:0.78\n",
      "epoch: 97, loss: 0.022914, val_acc:0.77, test_acc:0.79\n",
      "epoch: 98, loss: 0.023307, val_acc:0.74, test_acc:0.76\n",
      "epoch: 99, loss: 0.022807, val_acc:0.81, test_acc:0.8\n"
     ]
    }
   ],
   "source": [
    "#ResidualAttentionalGNN3\n",
    "%run main --alpha 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd83da0-5924-4c0d-b517-896fcc5195cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1078\n",
      "dataset loaded successfully! HCPGender\n",
      "dataset HCPGender loaded with train 754 val 108 test 216 splits\n",
      "ResidualAttentionalGNN3(\n",
      "  (aggr): MeanAggregation()\n",
      "  (convs): ModuleList(\n",
      "    (0-2): 3 x GNNWithAttention()\n",
      "  )\n",
      "  (bn): BatchNorm1d(998504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=998600, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters is: 65945458\n",
      "epoch: 0, loss: 0.04822, val_acc:0.52, test_acc:0.57\n",
      "epoch: 1, loss: 0.046745, val_acc:0.54, test_acc:0.63\n",
      "epoch: 2, loss: 0.043564, val_acc:0.59, test_acc:0.67\n",
      "epoch: 3, loss: 0.040831, val_acc:0.62, test_acc:0.68\n",
      "epoch: 4, loss: 0.041893, val_acc:0.66, test_acc:0.68\n",
      "epoch: 5, loss: 0.041501, val_acc:0.63, test_acc:0.65\n",
      "epoch: 6, loss: 0.040456, val_acc:0.69, test_acc:0.67\n",
      "epoch: 7, loss: 0.040062, val_acc:0.71, test_acc:0.76\n",
      "epoch: 8, loss: 0.0395, val_acc:0.65, test_acc:0.69\n",
      "epoch: 9, loss: 0.038773, val_acc:0.67, test_acc:0.69\n",
      "epoch: 10, loss: 0.03985, val_acc:0.66, test_acc:0.68\n",
      "epoch: 11, loss: 0.037539, val_acc:0.63, test_acc:0.67\n",
      "epoch: 12, loss: 0.037108, val_acc:0.69, test_acc:0.74\n",
      "epoch: 13, loss: 0.034793, val_acc:0.68, test_acc:0.69\n",
      "epoch: 14, loss: 0.038638, val_acc:0.72, test_acc:0.71\n",
      "epoch: 15, loss: 0.036179, val_acc:0.67, test_acc:0.69\n",
      "epoch: 16, loss: 0.035411, val_acc:0.66, test_acc:0.7\n",
      "epoch: 17, loss: 0.035088, val_acc:0.71, test_acc:0.75\n",
      "epoch: 18, loss: 0.035296, val_acc:0.74, test_acc:0.77\n",
      "epoch: 19, loss: 0.033406, val_acc:0.73, test_acc:0.74\n",
      "epoch: 20, loss: 0.03453, val_acc:0.79, test_acc:0.78\n",
      "epoch: 21, loss: 0.034884, val_acc:0.77, test_acc:0.77\n",
      "epoch: 22, loss: 0.034599, val_acc:0.75, test_acc:0.74\n",
      "epoch: 23, loss: 0.034151, val_acc:0.74, test_acc:0.8\n",
      "epoch: 24, loss: 0.033886, val_acc:0.77, test_acc:0.77\n",
      "epoch: 25, loss: 0.034302, val_acc:0.76, test_acc:0.8\n",
      "epoch: 26, loss: 0.033442, val_acc:0.74, test_acc:0.75\n",
      "epoch: 27, loss: 0.033355, val_acc:0.77, test_acc:0.8\n",
      "epoch: 28, loss: 0.03296, val_acc:0.73, test_acc:0.76\n",
      "epoch: 29, loss: 0.032314, val_acc:0.74, test_acc:0.76\n",
      "epoch: 30, loss: 0.031676, val_acc:0.69, test_acc:0.73\n",
      "epoch: 31, loss: 0.031593, val_acc:0.73, test_acc:0.74\n",
      "epoch: 32, loss: 0.032056, val_acc:0.79, test_acc:0.79\n",
      "epoch: 33, loss: 0.032501, val_acc:0.8, test_acc:0.79\n",
      "epoch: 34, loss: 0.031971, val_acc:0.71, test_acc:0.7\n",
      "epoch: 35, loss: 0.031308, val_acc:0.66, test_acc:0.68\n",
      "epoch: 36, loss: 0.032559, val_acc:0.69, test_acc:0.71\n",
      "epoch: 37, loss: 0.030628, val_acc:0.76, test_acc:0.77\n",
      "epoch: 38, loss: 0.031257, val_acc:0.73, test_acc:0.76\n",
      "epoch: 39, loss: 0.030102, val_acc:0.67, test_acc:0.72\n",
      "epoch: 40, loss: 0.030329, val_acc:0.69, test_acc:0.72\n",
      "epoch: 41, loss: 0.029919, val_acc:0.8, test_acc:0.8\n",
      "epoch: 42, loss: 0.03081, val_acc:0.74, test_acc:0.74\n",
      "epoch: 43, loss: 0.030191, val_acc:0.8, test_acc:0.77\n",
      "epoch: 44, loss: 0.03056, val_acc:0.76, test_acc:0.74\n",
      "epoch: 45, loss: 0.030505, val_acc:0.81, test_acc:0.81\n",
      "epoch: 46, loss: 0.030316, val_acc:0.81, test_acc:0.8\n",
      "epoch: 47, loss: 0.028532, val_acc:0.7, test_acc:0.74\n",
      "epoch: 48, loss: 0.029704, val_acc:0.76, test_acc:0.76\n",
      "epoch: 49, loss: 0.029793, val_acc:0.66, test_acc:0.72\n",
      "epoch: 50, loss: 0.029076, val_acc:0.71, test_acc:0.74\n",
      "epoch: 51, loss: 0.028238, val_acc:0.7, test_acc:0.72\n",
      "epoch: 52, loss: 0.028308, val_acc:0.77, test_acc:0.79\n",
      "epoch: 53, loss: 0.029221, val_acc:0.74, test_acc:0.75\n",
      "epoch: 54, loss: 0.028316, val_acc:0.72, test_acc:0.75\n",
      "epoch: 55, loss: 0.028389, val_acc:0.81, test_acc:0.79\n",
      "epoch: 56, loss: 0.028284, val_acc:0.71, test_acc:0.73\n",
      "epoch: 57, loss: 0.028147, val_acc:0.69, test_acc:0.73\n",
      "epoch: 58, loss: 0.027648, val_acc:0.69, test_acc:0.73\n",
      "epoch: 59, loss: 0.027691, val_acc:0.77, test_acc:0.77\n",
      "epoch: 60, loss: 0.028109, val_acc:0.69, test_acc:0.69\n",
      "epoch: 61, loss: 0.027544, val_acc:0.77, test_acc:0.76\n",
      "epoch: 62, loss: 0.027312, val_acc:0.74, test_acc:0.77\n",
      "epoch: 63, loss: 0.027251, val_acc:0.77, test_acc:0.77\n",
      "epoch: 64, loss: 0.028091, val_acc:0.71, test_acc:0.73\n",
      "epoch: 65, loss: 0.027939, val_acc:0.73, test_acc:0.73\n",
      "epoch: 66, loss: 0.026543, val_acc:0.69, test_acc:0.72\n",
      "epoch: 67, loss: 0.026462, val_acc:0.81, test_acc:0.79\n",
      "epoch: 68, loss: 0.02634, val_acc:0.73, test_acc:0.74\n",
      "epoch: 69, loss: 0.027427, val_acc:0.69, test_acc:0.74\n",
      "epoch: 70, loss: 0.026539, val_acc:0.7, test_acc:0.75\n",
      "epoch: 71, loss: 0.026169, val_acc:0.75, test_acc:0.75\n",
      "epoch: 72, loss: 0.02718, val_acc:0.78, test_acc:0.76\n",
      "epoch: 73, loss: 0.02582, val_acc:0.69, test_acc:0.73\n",
      "epoch: 74, loss: 0.026871, val_acc:0.69, test_acc:0.75\n",
      "epoch: 75, loss: 0.025733, val_acc:0.81, test_acc:0.78\n",
      "epoch: 76, loss: 0.02702, val_acc:0.77, test_acc:0.78\n",
      "epoch: 77, loss: 0.026068, val_acc:0.74, test_acc:0.77\n",
      "epoch: 78, loss: 0.027428, val_acc:0.71, test_acc:0.73\n",
      "epoch: 79, loss: 0.025985, val_acc:0.74, test_acc:0.74\n",
      "epoch: 80, loss: 0.025772, val_acc:0.75, test_acc:0.76\n",
      "epoch: 81, loss: 0.02584, val_acc:0.77, test_acc:0.79\n",
      "epoch: 82, loss: 0.025257, val_acc:0.76, test_acc:0.76\n",
      "epoch: 83, loss: 0.025227, val_acc:0.74, test_acc:0.77\n",
      "epoch: 84, loss: 0.025809, val_acc:0.79, test_acc:0.8\n",
      "epoch: 85, loss: 0.024635, val_acc:0.79, test_acc:0.79\n",
      "epoch: 86, loss: 0.024181, val_acc:0.68, test_acc:0.73\n",
      "epoch: 87, loss: 0.024881, val_acc:0.74, test_acc:0.76\n",
      "epoch: 88, loss: 0.025448, val_acc:0.81, test_acc:0.77\n",
      "epoch: 89, loss: 0.025905, val_acc:0.79, test_acc:0.79\n",
      "epoch: 90, loss: 0.024601, val_acc:0.78, test_acc:0.8\n",
      "epoch: 91, loss: 0.024861, val_acc:0.78, test_acc:0.78\n",
      "epoch: 92, loss: 0.024218, val_acc:0.79, test_acc:0.77\n",
      "epoch: 93, loss: 0.024393, val_acc:0.8, test_acc:0.75\n",
      "epoch: 94, loss: 0.024195, val_acc:0.79, test_acc:0.78\n",
      "epoch: 95, loss: 0.023282, val_acc:0.78, test_acc:0.75\n",
      "epoch: 96, loss: 0.024177, val_acc:0.82, test_acc:0.81\n",
      "epoch: 97, loss: 0.023216, val_acc:0.82, test_acc:0.81\n",
      "epoch: 98, loss: 0.023694, val_acc:0.77, test_acc:0.77\n",
      "epoch: 99, loss: 0.023578, val_acc:0.8, test_acc:0.81\n"
     ]
    }
   ],
   "source": [
    "#ResidualAttentionalGNN3\n",
    "%run main --alpha 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf74a77-08ab-4bc5-bd11-f7fa462fb6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304a015-396e-4122-8b14-7b63d876dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResidualGNNsWithInputAttention\n",
    "%run main --alpha 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1bcc6-718e-48b2-8e5d-598d28f6bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResidualGNNsWithSelfAttention\n",
    "%run main --alpha 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b69741-422e-4970-a2ed-fec2a0b8c374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1078\n",
      "dataset loaded successfully! HCPGender\n",
      "dataset HCPGender loaded with train 754 val 108 test 216 splits\n",
      "ResidualGNNs2(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1000, 32)\n",
      "    (1-2): 2 x GCNConv(32, 32)\n",
      "  )\n",
      "  (bn): BatchNorm1d(499500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attention): Linear(in_features=96, out_features=1, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=499501, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters is: 33005019\n",
      "epoch: 0, loss: 0.046773, val_acc:0.48, test_acc:0.52\n",
      "epoch: 1, loss: 0.045463, val_acc:0.55, test_acc:0.6\n",
      "epoch: 2, loss: 0.044505, val_acc:0.55, test_acc:0.61\n",
      "epoch: 3, loss: 0.043034, val_acc:0.59, test_acc:0.63\n",
      "epoch: 4, loss: 0.043305, val_acc:0.59, test_acc:0.64\n",
      "epoch: 5, loss: 0.042777, val_acc:0.62, test_acc:0.65\n",
      "epoch: 6, loss: 0.043531, val_acc:0.64, test_acc:0.68\n",
      "epoch: 7, loss: 0.041419, val_acc:0.58, test_acc:0.65\n",
      "epoch: 8, loss: 0.040691, val_acc:0.64, test_acc:0.7\n",
      "epoch: 9, loss: 0.043606, val_acc:0.65, test_acc:0.71\n",
      "epoch: 10, loss: 0.041921, val_acc:0.67, test_acc:0.74\n",
      "epoch: 11, loss: 0.041025, val_acc:0.66, test_acc:0.71\n",
      "epoch: 12, loss: 0.040418, val_acc:0.58, test_acc:0.69\n",
      "epoch: 13, loss: 0.039399, val_acc:0.65, test_acc:0.73\n",
      "epoch: 14, loss: 0.040072, val_acc:0.66, test_acc:0.73\n",
      "epoch: 15, loss: 0.041186, val_acc:0.66, test_acc:0.74\n",
      "epoch: 16, loss: 0.040293, val_acc:0.6, test_acc:0.68\n",
      "epoch: 17, loss: 0.040433, val_acc:0.62, test_acc:0.69\n",
      "epoch: 18, loss: 0.039197, val_acc:0.68, test_acc:0.75\n",
      "epoch: 19, loss: 0.039265, val_acc:0.69, test_acc:0.74\n",
      "epoch: 20, loss: 0.040288, val_acc:0.68, test_acc:0.7\n",
      "epoch: 21, loss: 0.038875, val_acc:0.69, test_acc:0.71\n",
      "epoch: 22, loss: 0.037377, val_acc:0.7, test_acc:0.79\n",
      "epoch: 23, loss: 0.037459, val_acc:0.69, test_acc:0.81\n",
      "epoch: 24, loss: 0.038672, val_acc:0.73, test_acc:0.81\n",
      "epoch: 25, loss: 0.038751, val_acc:0.7, test_acc:0.75\n",
      "epoch: 26, loss: 0.037513, val_acc:0.65, test_acc:0.75\n",
      "epoch: 27, loss: 0.037004, val_acc:0.79, test_acc:0.82\n",
      "epoch: 28, loss: 0.03717, val_acc:0.76, test_acc:0.82\n",
      "epoch: 29, loss: 0.036656, val_acc:0.69, test_acc:0.79\n",
      "epoch: 30, loss: 0.036559, val_acc:0.65, test_acc:0.76\n",
      "epoch: 31, loss: 0.037064, val_acc:0.72, test_acc:0.81\n",
      "epoch: 32, loss: 0.036576, val_acc:0.73, test_acc:0.8\n",
      "epoch: 33, loss: 0.036862, val_acc:0.74, test_acc:0.78\n",
      "epoch: 34, loss: 0.03645, val_acc:0.75, test_acc:0.81\n",
      "epoch: 35, loss: 0.036481, val_acc:0.68, test_acc:0.76\n",
      "epoch: 36, loss: 0.036322, val_acc:0.8, test_acc:0.81\n",
      "epoch: 37, loss: 0.037084, val_acc:0.79, test_acc:0.83\n",
      "epoch: 38, loss: 0.036552, val_acc:0.71, test_acc:0.77\n",
      "epoch: 39, loss: 0.03598, val_acc:0.69, test_acc:0.78\n",
      "epoch: 40, loss: 0.035081, val_acc:0.74, test_acc:0.82\n",
      "epoch: 41, loss: 0.035444, val_acc:0.65, test_acc:0.75\n",
      "epoch: 42, loss: 0.035102, val_acc:0.77, test_acc:0.83\n",
      "epoch: 43, loss: 0.035296, val_acc:0.77, test_acc:0.83\n",
      "epoch: 44, loss: 0.034176, val_acc:0.72, test_acc:0.81\n",
      "epoch: 45, loss: 0.034804, val_acc:0.72, test_acc:0.79\n",
      "epoch: 46, loss: 0.033745, val_acc:0.74, test_acc:0.8\n",
      "epoch: 47, loss: 0.035371, val_acc:0.8, test_acc:0.85\n",
      "epoch: 48, loss: 0.033963, val_acc:0.76, test_acc:0.83\n",
      "epoch: 49, loss: 0.034887, val_acc:0.8, test_acc:0.84\n",
      "epoch: 50, loss: 0.033809, val_acc:0.78, test_acc:0.83\n",
      "epoch: 51, loss: 0.033769, val_acc:0.77, test_acc:0.83\n",
      "epoch: 52, loss: 0.034272, val_acc:0.8, test_acc:0.86\n",
      "epoch: 53, loss: 0.033957, val_acc:0.81, test_acc:0.86\n",
      "epoch: 54, loss: 0.031857, val_acc:0.8, test_acc:0.85\n",
      "epoch: 55, loss: 0.033091, val_acc:0.79, test_acc:0.84\n",
      "epoch: 56, loss: 0.034233, val_acc:0.8, test_acc:0.84\n",
      "epoch: 57, loss: 0.033039, val_acc:0.81, test_acc:0.86\n",
      "epoch: 58, loss: 0.033182, val_acc:0.77, test_acc:0.85\n",
      "epoch: 59, loss: 0.033415, val_acc:0.8, test_acc:0.85\n",
      "epoch: 60, loss: 0.032705, val_acc:0.84, test_acc:0.85\n",
      "epoch: 61, loss: 0.031931, val_acc:0.8, test_acc:0.86\n",
      "epoch: 62, loss: 0.032665, val_acc:0.8, test_acc:0.84\n",
      "epoch: 63, loss: 0.031972, val_acc:0.8, test_acc:0.82\n",
      "epoch: 64, loss: 0.031399, val_acc:0.8, test_acc:0.84\n",
      "epoch: 65, loss: 0.031043, val_acc:0.85, test_acc:0.87\n",
      "epoch: 66, loss: 0.031512, val_acc:0.8, test_acc:0.83\n",
      "epoch: 67, loss: 0.03079, val_acc:0.8, test_acc:0.86\n",
      "epoch: 68, loss: 0.031565, val_acc:0.81, test_acc:0.84\n",
      "epoch: 69, loss: 0.03089, val_acc:0.81, test_acc:0.86\n",
      "epoch: 70, loss: 0.031759, val_acc:0.83, test_acc:0.88\n",
      "epoch: 71, loss: 0.029615, val_acc:0.84, test_acc:0.88\n",
      "epoch: 72, loss: 0.031418, val_acc:0.81, test_acc:0.87\n",
      "epoch: 73, loss: 0.031056, val_acc:0.85, test_acc:0.86\n",
      "epoch: 74, loss: 0.030791, val_acc:0.83, test_acc:0.88\n",
      "epoch: 75, loss: 0.029164, val_acc:0.81, test_acc:0.85\n",
      "epoch: 76, loss: 0.030793, val_acc:0.8, test_acc:0.85\n",
      "epoch: 77, loss: 0.029581, val_acc:0.84, test_acc:0.89\n",
      "epoch: 78, loss: 0.029611, val_acc:0.8, test_acc:0.85\n",
      "epoch: 79, loss: 0.028393, val_acc:0.82, test_acc:0.86\n",
      "epoch: 80, loss: 0.03059, val_acc:0.83, test_acc:0.87\n",
      "epoch: 81, loss: 0.029784, val_acc:0.83, test_acc:0.88\n",
      "epoch: 82, loss: 0.029759, val_acc:0.79, test_acc:0.86\n",
      "epoch: 83, loss: 0.030238, val_acc:0.87, test_acc:0.89\n",
      "epoch: 84, loss: 0.030392, val_acc:0.81, test_acc:0.88\n",
      "epoch: 85, loss: 0.028684, val_acc:0.8, test_acc:0.84\n",
      "epoch: 86, loss: 0.02898, val_acc:0.82, test_acc:0.87\n",
      "epoch: 87, loss: 0.029115, val_acc:0.85, test_acc:0.88\n",
      "epoch: 88, loss: 0.029711, val_acc:0.79, test_acc:0.88\n",
      "epoch: 89, loss: 0.028345, val_acc:0.82, test_acc:0.88\n",
      "epoch: 90, loss: 0.027723, val_acc:0.84, test_acc:0.88\n",
      "epoch: 91, loss: 0.027228, val_acc:0.82, test_acc:0.87\n",
      "epoch: 92, loss: 0.028157, val_acc:0.83, test_acc:0.88\n",
      "epoch: 93, loss: 0.028075, val_acc:0.87, test_acc:0.9\n",
      "epoch: 94, loss: 0.027374, val_acc:0.82, test_acc:0.88\n",
      "epoch: 95, loss: 0.027231, val_acc:0.84, test_acc:0.86\n",
      "epoch: 96, loss: 0.027033, val_acc:0.8, test_acc:0.82\n",
      "epoch: 97, loss: 0.026911, val_acc:0.85, test_acc:0.88\n",
      "epoch: 98, loss: 0.026037, val_acc:0.85, test_acc:0.89\n",
      "epoch: 99, loss: 0.027119, val_acc:0.84, test_acc:0.89\n"
     ]
    }
   ],
   "source": [
    "%ResidualGNNs2\n",
    "%run main --alpha 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb0dead-569d-442c-8eb7-399899c70dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1078\n",
      "dataset loaded successfully! HCPGender\n",
      "dataset HCPGender loaded with train 754 val 108 test 216 splits\n",
      "ResidualGNNs2(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1000, 32)\n",
      "    (1-2): 2 x GCNConv(32, 32)\n",
      "  )\n",
      "  (bn): BatchNorm1d(499500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attention): Linear(in_features=96, out_features=1, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=499501, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters is: 33005019\n",
      "epoch: 0, loss: 0.046482, val_acc:0.57, test_acc:0.61\n",
      "epoch: 1, loss: 0.044804, val_acc:0.58, test_acc:0.64\n",
      "epoch: 2, loss: 0.043804, val_acc:0.62, test_acc:0.66\n",
      "epoch: 3, loss: 0.043167, val_acc:0.6, test_acc:0.64\n",
      "epoch: 4, loss: 0.043286, val_acc:0.55, test_acc:0.63\n",
      "epoch: 5, loss: 0.044521, val_acc:0.58, test_acc:0.64\n",
      "epoch: 6, loss: 0.044087, val_acc:0.65, test_acc:0.68\n",
      "epoch: 7, loss: 0.042771, val_acc:0.6, test_acc:0.64\n",
      "epoch: 8, loss: 0.043647, val_acc:0.67, test_acc:0.75\n",
      "epoch: 9, loss: 0.04115, val_acc:0.66, test_acc:0.7\n",
      "epoch: 10, loss: 0.041593, val_acc:0.6, test_acc:0.68\n",
      "epoch: 11, loss: 0.040788, val_acc:0.62, test_acc:0.76\n",
      "epoch: 12, loss: 0.041357, val_acc:0.67, test_acc:0.78\n",
      "epoch: 13, loss: 0.039201, val_acc:0.65, test_acc:0.74\n",
      "epoch: 14, loss: 0.039851, val_acc:0.66, test_acc:0.76\n",
      "epoch: 15, loss: 0.038973, val_acc:0.68, test_acc:0.73\n",
      "epoch: 16, loss: 0.03988, val_acc:0.73, test_acc:0.83\n",
      "epoch: 17, loss: 0.039731, val_acc:0.68, test_acc:0.78\n",
      "epoch: 18, loss: 0.038019, val_acc:0.76, test_acc:0.78\n",
      "epoch: 19, loss: 0.038842, val_acc:0.7, test_acc:0.77\n",
      "epoch: 20, loss: 0.039981, val_acc:0.69, test_acc:0.75\n",
      "epoch: 21, loss: 0.038563, val_acc:0.64, test_acc:0.66\n",
      "epoch: 22, loss: 0.037782, val_acc:0.7, test_acc:0.81\n",
      "epoch: 23, loss: 0.038658, val_acc:0.79, test_acc:0.83\n",
      "epoch: 24, loss: 0.037917, val_acc:0.69, test_acc:0.79\n",
      "epoch: 25, loss: 0.039771, val_acc:0.67, test_acc:0.79\n",
      "epoch: 26, loss: 0.036638, val_acc:0.76, test_acc:0.82\n",
      "epoch: 27, loss: 0.03884, val_acc:0.74, test_acc:0.81\n",
      "epoch: 28, loss: 0.038762, val_acc:0.7, test_acc:0.77\n",
      "epoch: 29, loss: 0.037011, val_acc:0.69, test_acc:0.75\n",
      "epoch: 30, loss: 0.037049, val_acc:0.73, test_acc:0.79\n",
      "epoch: 31, loss: 0.037076, val_acc:0.74, test_acc:0.8\n",
      "epoch: 32, loss: 0.036834, val_acc:0.71, test_acc:0.78\n",
      "epoch: 33, loss: 0.036782, val_acc:0.69, test_acc:0.78\n",
      "epoch: 34, loss: 0.03588, val_acc:0.75, test_acc:0.81\n",
      "epoch: 35, loss: 0.036677, val_acc:0.75, test_acc:0.8\n",
      "epoch: 36, loss: 0.036262, val_acc:0.75, test_acc:0.8\n",
      "epoch: 37, loss: 0.036026, val_acc:0.75, test_acc:0.81\n",
      "epoch: 38, loss: 0.034762, val_acc:0.82, test_acc:0.83\n",
      "epoch: 39, loss: 0.035418, val_acc:0.75, test_acc:0.85\n",
      "epoch: 40, loss: 0.034512, val_acc:0.75, test_acc:0.82\n",
      "epoch: 41, loss: 0.035345, val_acc:0.74, test_acc:0.81\n",
      "epoch: 42, loss: 0.035457, val_acc:0.77, test_acc:0.85\n",
      "epoch: 43, loss: 0.035463, val_acc:0.75, test_acc:0.82\n",
      "epoch: 44, loss: 0.034026, val_acc:0.85, test_acc:0.88\n",
      "epoch: 45, loss: 0.03438, val_acc:0.78, test_acc:0.81\n",
      "epoch: 46, loss: 0.034673, val_acc:0.81, test_acc:0.85\n",
      "epoch: 47, loss: 0.034226, val_acc:0.69, test_acc:0.75\n",
      "epoch: 48, loss: 0.032618, val_acc:0.82, test_acc:0.85\n",
      "epoch: 49, loss: 0.034916, val_acc:0.74, test_acc:0.8\n",
      "epoch: 50, loss: 0.034713, val_acc:0.77, test_acc:0.86\n",
      "epoch: 51, loss: 0.03472, val_acc:0.74, test_acc:0.81\n",
      "epoch: 52, loss: 0.033545, val_acc:0.79, test_acc:0.85\n",
      "epoch: 53, loss: 0.034441, val_acc:0.76, test_acc:0.81\n",
      "epoch: 54, loss: 0.033187, val_acc:0.8, test_acc:0.82\n",
      "epoch: 55, loss: 0.032336, val_acc:0.84, test_acc:0.87\n",
      "epoch: 56, loss: 0.031843, val_acc:0.81, test_acc:0.83\n",
      "epoch: 57, loss: 0.033028, val_acc:0.79, test_acc:0.84\n",
      "epoch: 58, loss: 0.032814, val_acc:0.81, test_acc:0.85\n",
      "epoch: 59, loss: 0.031452, val_acc:0.76, test_acc:0.82\n",
      "epoch: 60, loss: 0.032176, val_acc:0.83, test_acc:0.82\n",
      "epoch: 61, loss: 0.032695, val_acc:0.79, test_acc:0.85\n",
      "epoch: 62, loss: 0.032626, val_acc:0.79, test_acc:0.84\n",
      "epoch: 63, loss: 0.031811, val_acc:0.78, test_acc:0.83\n",
      "epoch: 64, loss: 0.031642, val_acc:0.81, test_acc:0.84\n",
      "epoch: 65, loss: 0.031719, val_acc:0.78, test_acc:0.83\n",
      "epoch: 66, loss: 0.031739, val_acc:0.78, test_acc:0.83\n",
      "epoch: 67, loss: 0.031967, val_acc:0.81, test_acc:0.84\n",
      "epoch: 68, loss: 0.03088, val_acc:0.75, test_acc:0.81\n",
      "epoch: 69, loss: 0.032742, val_acc:0.78, test_acc:0.84\n",
      "epoch: 70, loss: 0.031712, val_acc:0.8, test_acc:0.87\n",
      "epoch: 71, loss: 0.031013, val_acc:0.81, test_acc:0.87\n",
      "epoch: 72, loss: 0.030968, val_acc:0.83, test_acc:0.88\n",
      "epoch: 73, loss: 0.030534, val_acc:0.81, test_acc:0.85\n",
      "epoch: 74, loss: 0.029209, val_acc:0.86, test_acc:0.89\n",
      "epoch: 75, loss: 0.030275, val_acc:0.79, test_acc:0.85\n",
      "epoch: 76, loss: 0.029208, val_acc:0.86, test_acc:0.9\n",
      "epoch: 77, loss: 0.029487, val_acc:0.82, test_acc:0.87\n",
      "epoch: 78, loss: 0.029304, val_acc:0.81, test_acc:0.87\n",
      "epoch: 79, loss: 0.030226, val_acc:0.83, test_acc:0.86\n",
      "epoch: 80, loss: 0.029691, val_acc:0.82, test_acc:0.88\n",
      "epoch: 81, loss: 0.030054, val_acc:0.8, test_acc:0.86\n",
      "epoch: 82, loss: 0.030231, val_acc:0.85, test_acc:0.87\n",
      "epoch: 83, loss: 0.029753, val_acc:0.84, test_acc:0.86\n",
      "epoch: 84, loss: 0.028797, val_acc:0.83, test_acc:0.87\n",
      "epoch: 85, loss: 0.029261, val_acc:0.83, test_acc:0.88\n",
      "epoch: 86, loss: 0.027549, val_acc:0.81, test_acc:0.85\n",
      "epoch: 87, loss: 0.029133, val_acc:0.8, test_acc:0.85\n",
      "epoch: 88, loss: 0.028677, val_acc:0.84, test_acc:0.85\n",
      "epoch: 89, loss: 0.027832, val_acc:0.81, test_acc:0.87\n",
      "epoch: 90, loss: 0.028428, val_acc:0.77, test_acc:0.84\n",
      "epoch: 91, loss: 0.027359, val_acc:0.84, test_acc:0.88\n",
      "epoch: 92, loss: 0.027725, val_acc:0.85, test_acc:0.86\n",
      "epoch: 93, loss: 0.027312, val_acc:0.87, test_acc:0.9\n",
      "epoch: 94, loss: 0.02757, val_acc:0.83, test_acc:0.87\n",
      "epoch: 95, loss: 0.026845, val_acc:0.8, test_acc:0.84\n",
      "epoch: 96, loss: 0.027786, val_acc:0.8, test_acc:0.84\n",
      "epoch: 97, loss: 0.027228, val_acc:0.82, test_acc:0.89\n",
      "epoch: 98, loss: 0.02618, val_acc:0.79, test_acc:0.83\n",
      "epoch: 99, loss: 0.026859, val_acc:0.77, test_acc:0.84\n"
     ]
    }
   ],
   "source": [
    "#ResidualGNNs2\n",
    "%run main --alpha 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cbfc6-e60c-4e4d-913c-e1f91b88ddee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
