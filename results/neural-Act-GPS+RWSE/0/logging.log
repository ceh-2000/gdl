[*] Run ID 0: seed=0, split_index=0
    Starting now: 2024-02-14 20:48:22.142667
[*] Loaded dataset 'HCPActivity' from 'PyG-NeuroGraphDataset':
  Data(x=[2977200, 400], edge_index=[2, 52318216], y=[7443])
  undirected: True
  num graphs: 7443
  avg num_nodes/graph: 400
  num node features: 400
  num edge features: 1
  num classes: 7
Parsed RWSE PE kernel times / steps: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
Precomputing Positional Encoding statistics: ['RWSE'] for all graphs...
  ...estimated to be undirected: True
Done! Took 00:02:06.33
GraphGymModule(
  (model): GPSModel(
    (encoder): FeatureEncoder(
      (node_encoder): RWSENodeEncoder(
        (linear_x): Linear(in_features=400, out_features=48, bias=True)
        (raw_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (pe_encoder): Linear(in_features=16, out_features=16, bias=True)
      )
      (edge_encoder): LinearEdgeEncoder(
        (encoder): Linear(in_features=1, out_features=64, bias=True)
      )
    )
    (layers): Sequential(
      (0): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (1): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (2): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (3): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (4): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (5): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (6): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (7): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (8): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (9): GPSLayer(
        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)
        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (post_mp): GNNGraphHead(
      (layer_post_mp): MLP(
        (model): Sequential(
          (0): GeneralMultiLayer(
            (Layer_0): GeneralLayer(
              (layer): Linear(
                (model): Linear(64, 64, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
            (Layer_1): GeneralLayer(
              (layer): Linear(
                (model): Linear(64, 64, bias=True)
              )
              (post_layer): Sequential(
                (0): ReLU()
              )
            )
          )
          (1): Linear(
            (model): Linear(64, 7, bias=True)
          )
        )
      )
    )
  )
)
accelerator: cuda
benchmark: False
bn:
  eps: 1e-05
  mom: 0.1
cfg_dest: config.yaml
custom_metrics: []
dataset:
  cache_load: False
  cache_save: False
  dir: ./datasets
  edge_dim: 128
  edge_encoder: True
  edge_encoder_bn: False
  edge_encoder_name: LinearEdge
  edge_encoder_num_types: 0
  edge_message_ratio: 0.8
  edge_negative_sampling_ratio: 1.0
  edge_train_mode: all
  encoder: True
  encoder_bn: True
  encoder_dim: 128
  encoder_name: db
  format: PyG-NeuroGraphDataset
  infer_link_label: None
  label_column: none
  label_table: none
  location: local
  name: HCPActivity
  node_encoder: True
  node_encoder_bn: False
  node_encoder_name: RWSE
  node_encoder_num_types: 0
  remove_feature: False
  resample_disjoint: False
  resample_negative: False
  shuffle_split: True
  slic_compactness: 10
  split: [0.8, 0.1, 0.1]
  split_dir: ./splits
  split_index: 0
  split_mode: random
  task: graph
  task_type: classification
  to_undirected: False
  transductive: False
  transform: none
  tu_simple: True
devices: 1
example_arg: example
example_group:
  example_arg: example
gnn:
  act: relu
  agg: mean
  att_final_linear: False
  att_final_linear_bn: False
  att_heads: 1
  batchnorm: False
  clear_feature: True
  dim_edge: 64
  dim_inner: 64
  dropout: 0.0
  head: graph
  keep_edge: 0.5
  l2norm: True
  layer_type: generalconv
  layers_mp: 2
  layers_post_mp: 3
  layers_pre_mp: 0
  msg_direction: single
  normalize_adj: False
  residual: False
  self_msg: concat
  skip_every: 1
  stage_type: stack
gpu_mem: False
graphormer:
  attention_dropout: 0.0
  dropout: 0.0
  embed_dim: 80
  input_dropout: 0.0
  mlp_dropout: 0.0
  num_heads: 4
  num_layers: 6
  use_graph_token: True
gt:
  activation: relu
  attn_dropout: 0.5
  batch_norm: True
  bigbird:
    add_cross_attention: False
    attention_type: block_sparse
    block_size: 3
    chunk_size_feed_forward: 0
    hidden_act: relu
    is_decoder: False
    layer_norm_eps: 1e-06
    max_position_embeddings: 128
    num_random_blocks: 3
    use_bias: False
  dim_edge: None
  dim_hidden: 64
  dropout: 0.0
  full_graph: True
  gamma: 1e-05
  layer_norm: False
  layer_type: CustomGatedGCN+Transformer
  layers: 10
  n_heads: 4
  pna_degrees: []
  residual: True
  secondary_edges: full_graph
mem:
  inplace: False
metric_agg: argmax
metric_best: accuracy
model:
  edge_decoding: dot
  graph_pooling: mean
  loss_fun: cross_entropy
  match_upper: True
  size_average: mean
  thresh: 0.5
  type: GPSModel
name_tag: 
num_threads: 6
num_workers: 0
optim:
  base_lr: 0.001
  batch_accumulation: 1
  clip_grad_norm: True
  clip_grad_norm_value: 1.0
  lr_decay: 0.1
  max_epoch: 10
  min_lr: 0.0
  momentum: 0.9
  num_warmup_epochs: 5
  optimizer: adamW
  reduce_factor: 0.1
  schedule_patience: 10
  scheduler: cosine_with_warmup
  steps: [30, 60, 90]
  weight_decay: 1e-05
out_dir: results\neural-Act-GPS+RWSE
posenc_ERE:
  accuracy: 0.1
  dim_pe: 16
  enable: False
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_ERN:
  accuracy: 0.1
  dim_pe: 16
  enable: False
  er_dim: none
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_ElstaticSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: range(10)
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_EquivStableLapPE:
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  raw_norm_type: none
posenc_GraphormerBias:
  dim_pe: 0
  enable: False
  node_degrees_only: False
  num_in_degrees: None
  num_out_degrees: None
  num_spatial_types: None
posenc_HKdiagSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_LapPE:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_RWSE:
  dim_pe: 16
  enable: True
  kernel:
    times: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
    times_func: range(1,17)
  layers: 3
  model: Linear
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: BatchNorm
posenc_SignNet:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  model: none
  n_heads: 4
  pass_as_var: False
  phi_hidden_dim: 64
  phi_out_dim: 4
  post_layers: 0
  raw_norm_type: none
prep:
  add_edge_index: True
  add_reverse_edges: True
  add_self_loops: False
  dist_cutoff: 510
  dist_enable: False
  exp: False
  exp_algorithm: Random-d
  exp_count: 1
  exp_deg: 5
  exp_max_num_iters: 100
  layer_edge_indices_dir: None
  num_virt_node: 0
  train_percent: 0.6
  use_exp_edges: True
pretrained:
  dir: 
  freeze_main: False
  reset_prediction_head: True
print: both
round: 5
run_dir: results\neural-Act-GPS+RWSE\0
run_id: 0
run_multiple_splits: []
seed: 0
share:
  dim_in: 400
  dim_out: 7
  num_splits: 3
tensorboard_agg: True
tensorboard_each_run: True
train:
  auto_resume: False
  batch_size: 16
  ckpt_best: False
  ckpt_clean: True
  ckpt_period: 100
  enable_ckpt: True
  epoch_resume: -1
  eval_period: 1
  iter_per_epoch: 32
  mode: custom
  neighbor_sizes: [20, 15, 10, 5]
  node_per_graph: 32
  radius: extend
  sample_node: False
  sampler: full_batch
  skip_train_eval: False
  walk_length: 4
val:
  node_per_graph: 32
  radius: extend
  sample_node: False
  sampler: full_batch
view_emb: False
wandb:
  entity: gtransformers
  name: 
  project: neural
  use: False
Num parameters: 575015
Start from epoch 0
train: {'epoch': 0, 'time_epoch': 100.5235, 'eta': 904.7115, 'eta_hours': 0.25131, 'loss': 1.94689485, 'lr': 0.0, 'params': 575015, 'time_iter': 0.2695, 'accuracy': 0.14965, 'f1': 0.07211, 'auc': 0.51637}
...computing epoch stats took: 0.42s
val: {'epoch': 0, 'time_epoch': 6.61436, 'loss': 1.94562449, 'lr': 0, 'params': 575015, 'time_iter': 0.14073, 'accuracy': 0.15188, 'f1': 0.07822, 'auc': 0.52814}
...computing epoch stats took: 0.17s
test: {'epoch': 0, 'time_epoch': 6.61675, 'loss': 1.94980852, 'lr': 0, 'params': 575015, 'time_iter': 0.14078, 'accuracy': 0.13826, 'f1': 0.06839, 'auc': 0.51239}
...computing epoch stats took: 0.03s
> Epoch 0: took 114.4s (avg 114.4s) | Best so far: epoch 0	train_loss: 1.9469 train_accuracy: 0.1497	val_loss: 1.9456 val_accuracy: 0.1519	test_loss: 1.9498 test_accuracy: 0.1383
train: {'epoch': 1, 'time_epoch': 96.29338, 'eta': 787.26751, 'eta_hours': 0.21869, 'loss': 1.71491993, 'lr': 0.0002, 'params': 575015, 'time_iter': 0.25816, 'accuracy': 0.57995, 'f1': 0.56893, 'auc': 0.89208}
...computing epoch stats took: 0.06s
val: {'epoch': 1, 'time_epoch': 5.83632, 'loss': 1.49663668, 'lr': 0, 'params': 575015, 'time_iter': 0.12418, 'accuracy': 0.83737, 'f1': 0.84048, 'auc': 0.97336}
...computing epoch stats took: 0.03s
test: {'epoch': 1, 'time_epoch': 5.98908, 'loss': 1.49531289, 'lr': 0, 'params': 575015, 'time_iter': 0.12743, 'accuracy': 0.82416, 'f1': 0.81494, 'auc': 0.97672}
...computing epoch stats took: 0.02s
> Epoch 1: took 108.3s (avg 111.3s) | Best so far: epoch 1	train_loss: 1.7149 train_accuracy: 0.5799	val_loss: 1.4966 val_accuracy: 0.8374	test_loss: 1.4953 test_accuracy: 0.8242
train: {'epoch': 2, 'time_epoch': 95.88711, 'eta': 682.97596, 'eta_hours': 0.18972, 'loss': 1.2853626, 'lr': 0.0004, 'params': 575015, 'time_iter': 0.25707, 'accuracy': 0.85539, 'f1': 0.85448, 'auc': 0.97079}
...computing epoch stats took: 0.03s
val: {'epoch': 2, 'time_epoch': 6.01995, 'loss': 1.12226002, 'lr': 0, 'params': 575015, 'time_iter': 0.12808, 'accuracy': 0.86828, 'f1': 0.86656, 'auc': 0.97804}
...computing epoch stats took: 0.02s
test: {'epoch': 2, 'time_epoch': 5.8659, 'loss': 1.11014365, 'lr': 0, 'params': 575015, 'time_iter': 0.12481, 'accuracy': 0.86577, 'f1': 0.86334, 'auc': 0.97874}
...computing epoch stats took: 0.03s
> Epoch 2: took 107.9s (avg 110.2s) | Best so far: epoch 2	train_loss: 1.2854 train_accuracy: 0.8554	val_loss: 1.1223 val_accuracy: 0.8683	test_loss: 1.1101 test_accuracy: 0.8658
train: {'epoch': 3, 'time_epoch': 94.57536, 'eta': 580.91902, 'eta_hours': 0.16137, 'loss': 0.90610736, 'lr': 0.0006, 'params': 575015, 'time_iter': 0.25355, 'accuracy': 0.89436, 'f1': 0.89365, 'auc': 0.9791}
val: {'epoch': 3, 'time_epoch': 5.61015, 'loss': 0.73078662, 'lr': 0, 'params': 575015, 'time_iter': 0.11936, 'accuracy': 0.91532, 'f1': 0.91799, 'auc': 0.98965}
test: {'epoch': 3, 'time_epoch': 5.97572, 'loss': 0.70947025, 'lr': 0, 'params': 575015, 'time_iter': 0.12714, 'accuracy': 0.93289, 'f1': 0.93371, 'auc': 0.98913}
> Epoch 3: took 106.3s (avg 109.2s) | Best so far: epoch 3	train_loss: 0.9061 train_accuracy: 0.8944	val_loss: 0.7308 val_accuracy: 0.9153	test_loss: 0.7095 test_accuracy: 0.9329
train: {'epoch': 4, 'time_epoch': 94.3734, 'eta': 481.65274, 'eta_hours': 0.13379, 'loss': 0.66072103, 'lr': 0.0008, 'params': 575015, 'time_iter': 0.25301, 'accuracy': 0.89604, 'f1': 0.89567, 'auc': 0.97726}
val: {'epoch': 4, 'time_epoch': 5.95527, 'loss': 0.66222956, 'lr': 0, 'params': 575015, 'time_iter': 0.12671, 'accuracy': 0.86425, 'f1': 0.86328, 'auc': 0.96952}
test: {'epoch': 4, 'time_epoch': 5.91099, 'loss': 0.55800585, 'lr': 0, 'params': 575015, 'time_iter': 0.12577, 'accuracy': 0.90604, 'f1': 0.90035, 'auc': 0.98013}
> Epoch 4: took 106.4s (avg 108.6s) | Best so far: epoch 3	train_loss: 0.9061 train_accuracy: 0.8944	val_loss: 0.7308 val_accuracy: 0.9153	test_loss: 0.7095 test_accuracy: 0.9329
train: {'epoch': 5, 'time_epoch': 95.7314, 'eta': 384.92276, 'eta_hours': 0.10692, 'loss': 0.49364927, 'lr': 0.001, 'params': 575015, 'time_iter': 0.25665, 'accuracy': 0.90578, 'f1': 0.9056, 'auc': 0.98144}
val: {'epoch': 5, 'time_epoch': 5.89695, 'loss': 0.53156066, 'lr': 0, 'params': 575015, 'time_iter': 0.12547, 'accuracy': 0.87366, 'f1': 0.87452, 'auc': 0.97619}
test: {'epoch': 5, 'time_epoch': 5.88964, 'loss': 0.49022585, 'lr': 0, 'params': 575015, 'time_iter': 0.12531, 'accuracy': 0.89799, 'f1': 0.89994, 'auc': 0.97505}
> Epoch 5: took 107.6s (avg 108.5s) | Best so far: epoch 3	train_loss: 0.9061 train_accuracy: 0.8944	val_loss: 0.7308 val_accuracy: 0.9153	test_loss: 0.7095 test_accuracy: 0.9329
train: {'epoch': 6, 'time_epoch': 95.20554, 'eta': 288.25272, 'eta_hours': 0.08007, 'loss': 0.3007809, 'lr': 0.00090451, 'params': 575015, 'time_iter': 0.25524, 'accuracy': 0.94928, 'f1': 0.94921, 'auc': 0.99303}
val: {'epoch': 6, 'time_epoch': 5.77708, 'loss': 0.28341929, 'lr': 0, 'params': 575015, 'time_iter': 0.12292, 'accuracy': 0.93683, 'f1': 0.93762, 'auc': 0.9961}
test: {'epoch': 6, 'time_epoch': 6.11284, 'loss': 0.33448371, 'lr': 0, 'params': 575015, 'time_iter': 0.13006, 'accuracy': 0.92752, 'f1': 0.92792, 'auc': 0.99042}
> Epoch 6: took 107.2s (avg 108.3s) | Best so far: epoch 6	train_loss: 0.3008 train_accuracy: 0.9493	val_loss: 0.2834 val_accuracy: 0.9368	test_loss: 0.3345 test_accuracy: 0.9275
train: {'epoch': 7, 'time_epoch': 96.8039, 'eta': 192.3484, 'eta_hours': 0.05343, 'loss': 0.21774153, 'lr': 0.00065451, 'params': 575015, 'time_iter': 0.25953, 'accuracy': 0.96187, 'f1': 0.96178, 'auc': 0.99515}
val: {'epoch': 7, 'time_epoch': 5.72598, 'loss': 0.2733709, 'lr': 0, 'params': 575015, 'time_iter': 0.12183, 'accuracy': 0.9422, 'f1': 0.94273, 'auc': 0.99309}
test: {'epoch': 7, 'time_epoch': 5.67354, 'loss': 0.28401506, 'lr': 0, 'params': 575015, 'time_iter': 0.12071, 'accuracy': 0.9396, 'f1': 0.93792, 'auc': 0.99241}
> Epoch 7: took 108.3s (avg 108.3s) | Best so far: epoch 7	train_loss: 0.2177 train_accuracy: 0.9619	val_loss: 0.2734 val_accuracy: 0.9422	test_loss: 0.2840 test_accuracy: 0.9396
train: {'epoch': 8, 'time_epoch': 94.74103, 'eta': 96.01496, 'eta_hours': 0.02667, 'loss': 0.14163512, 'lr': 0.00034549, 'params': 575015, 'time_iter': 0.254, 'accuracy': 0.98119, 'f1': 0.98117, 'auc': 0.99824}
val: {'epoch': 8, 'time_epoch': 5.77361, 'loss': 0.18680908, 'lr': 0, 'params': 575015, 'time_iter': 0.12284, 'accuracy': 0.96505, 'f1': 0.96526, 'auc': 0.99579}
test: {'epoch': 8, 'time_epoch': 5.77845, 'loss': 0.19718751, 'lr': 0, 'params': 575015, 'time_iter': 0.12295, 'accuracy': 0.95705, 'f1': 0.95636, 'auc': 0.9976}
> Epoch 8: took 106.4s (avg 108.1s) | Best so far: epoch 8	train_loss: 0.1416 train_accuracy: 0.9812	val_loss: 0.1868 val_accuracy: 0.9650	test_loss: 0.1972 test_accuracy: 0.9570
train: {'epoch': 9, 'time_epoch': 95.50757, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.10450313, 'lr': 9.549e-05, 'params': 575015, 'time_iter': 0.25605, 'accuracy': 0.9911, 'f1': 0.99107, 'auc': 0.99936}
val: {'epoch': 9, 'time_epoch': 5.76737, 'loss': 0.15298851, 'lr': 0, 'params': 575015, 'time_iter': 0.12271, 'accuracy': 0.97312, 'f1': 0.97352, 'auc': 0.99702}
test: {'epoch': 9, 'time_epoch': 5.76401, 'loss': 0.15575235, 'lr': 0, 'params': 575015, 'time_iter': 0.12264, 'accuracy': 0.97315, 'f1': 0.97252, 'auc': 0.99766}
> Epoch 9: took 107.1s (avg 108.0s) | Best so far: epoch 9	train_loss: 0.1045 train_accuracy: 0.9911	val_loss: 0.1530 val_accuracy: 0.9731	test_loss: 0.1558 test_accuracy: 0.9731
Avg time per epoch: 107.99s
Total train loop time: 0.30h
Task done, results saved in results\neural-Act-GPS+RWSE\0
Results aggregated across runs saved in results\neural-Act-GPS+RWSE\agg
[*] All done: 2024-02-14 21:08:38.642540
